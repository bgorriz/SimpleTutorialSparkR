---
title: "Tutorial SparkR"
authors: "Maria Pelaez & Bartek Skorulski"
output:
    html_document:
        toc: true
        highlight: zenburn
---

# Introduction

This is a short and simple tutorial of SparkR. We
introduce SparkR and we show simple examples how to do
an exploratory analysis.

More information you can find on the following pages.

- <https://spark.apache.org/docs/latest/api/R/index.html>
- <http://rmarkdown.rstudio.com>.
- <http://spark.apache.org>
- <http://www.r-project.org>
- <http://www.ggplot2.org>
- <https://cran.r-project.org/web/packages/pipeR>
- <https://spark.apache.org/docs/latest/api/R/index.html>


## What is R?

R is a __programming language__ that is very popular among Data
Scientist (_better Computer Scientist than average Statistician and
better Statistician than average Computer Scientist_).

### Magic of R

* Open Source
* Rich of statistical, graphics and general packages
* One can manipulate R objects directly in C, C++, Fortran, Java 
* It can produce publication quality documents

### Limitations of R

* Single threaded
* Everything has to fit in memory

### How do Data Scientists work with R?

<center>
<img src="images/pic2.png" height="200px"/> 
</center>

- Distributed Storages: Hadoop, Mesos, AWS S3, Cassandra...
- Framework: Hadoop MR, Hive, Pig,...
- Local Storages: CSV, database, ...



## What are Spark and SparkR?

**Spark**: Framework for cluster computing (you can use with Java, Scala, Python,...)
**SparkR** (spark+R): Framework for cluster computing using R

<center>
<img src="images/pic4.png" height="200px"/> 
</center>

## SparkR architecture

<center>
<img src="images/pic1.png" height="200px"/> 
</center>


# 1. SparkR initialization

Lets set variable that store location of spark,

```{r}
## SPARK_HOME <- Sys.getenv("SPARK_HOME")
SPARK_HOME <- "/home/bartek/programs/spark-1.5.2-bin-hadoop2.6/"
## SPARK_HOME <- "/Users/CotePelaez/Documents/spark-1.5.2-bin-hadoop2.6/"
```

## 1.1 Load SparR package

```{r LoadSparR}
.libPaths(c(.libPaths(), file.path(SPARK_HOME,"R/lib/")))
Sys.setenv('SPARKR_SUBMIT_ARGS'='"--packages" "com.databricks:spark-csv_2.10:1.2.0" "sparkr-shell"')
##library(SparkR, lib.loc = paste0(SPARK_HOME, "R/lib/"))
library(SparkR)#, lib.loc = paste0("/Users/CotePelaez/Documents/spark-1.5.2-bin-hadoop2.6/", "R/lib/"))
```


## 1.2 Load and install aditional packages

```{r LoadPackages}
library(rJava)
library(ggplot2)
library(pipeR)
library(whisker)
```


## 1.3 Initialization context

For educational reason Spark allows to be run on one local machine. We get that by assigning master to local machine. 

```{r}
sc <- sparkR.init(master = "local", sparkHome = SPARK_HOME)
sqlContext <- sparkRSQL.init(sc)
hiveContext <- sparkRHive.init(sc)
```

Now, we can get access to Spark UI at <http://localhost:4040>


# 2. Introducing DataFrame

## 2.1 Difference between **data.frame** and **DataFrame**

DataFrame is Spark object that allows to do the computations on distributed storages directly in R. 
We can create DataFrame object from standard data.frame as follows:

```{r DataFrame}
class(mtcars)
##df_mtcars <- createDataFrame(sqlContext, mtcars)
df_mtcars <- createDataFrame(hiveContext, mtcars)
class(df_mtcars)
df_mtcars
head(df_mtcars)
```

Note that this DataFrame object is not in the workspace of R; it is enterily in Spark.


## 2.2 Different ways to do simple aggregation

```{r Aggregation}

count(filter(df_mtcars, "cyl = 6"))

```

### Method chaining (piping)

```{r}
df_mtcars %>>%
  filter("cyl = 6") %>>%
  count
```

### Querying

```{r}
df_mtcars %>>%
  registerTempTable("mtcars")


hiveContext %>>%
  tables %>>%
  collect

sql(hiveContext, "select count(*) from mtcars where cyl = 6") %>>% collect
```

## 2.3 Lazy Execution

Spark does not execute calculations until we ask for results. Example:

```{r LazyExecution}
path1 <- "data/test.csv"
df1 <- read.df(hiveContext, path1,
               source = "com.databricks.spark.csv",
               header="true",
               inferSchema = "true")
class(df1)
head(df1)

p <- proc.time()
df1_store3 <-
  df1 %>>% 
  filter(df1$Store==3) 
proc.time()-p
 
p <- proc.time()
df1_store3 %>>%
  count
proc.time()-p

```

## 2.4 Cache

```{r}

df1_store3_cache <-
  df1 %>>% 
  filter(.$Store==3) %>>%
  cache

p <- proc.time()
df1_store3_cache %>>% count
proc.time()-p


# eliminating if you dont need more
df1_store3_cache %>>% unpersist()
```

## 2.3.Basic manipulations in SparkR

```{r}

states_properties_path <- "data/states_properties.csv"

states_division_path <- "data/states_division.csv"

sp_df <- read.df(hiveContext, states_properties_path,
               source = "com.databricks.spark.csv",
               header="true",
               inferSchema = "true")

sd_df <- read.df(hiveContext, states_division_path,
               source = "com.databricks.spark.csv",
               header="true",
               inferSchema = "true")

```


* `describe`
* `filter` 
* `select` 
* `distinct`
* `mutate` (`withColumn`)
* `collect`
* `join`

```{r}
sp_df %>>%
  describe %>>%
  collect
```

```{r}
sd_df %>>%
  select("state_division") %>>%
  distinct %>>%
  count
```

```{r}
sp_df %>>%
  mutate(Area_km2= (.$Area * 2.58999)) %>>%
  head

sp_df %>>%
  head

```

```{r}

s_df <-
  sp_df %>>%
  mutate(Area_km2= (.$Area * 2.58999)) %>>%
  join(sd_df, .$state_abb == sd_df$state_abb)
```

```{r}
d_df <-
  s_df %>>%
  groupBy("state_division") %>>%
  agg(max_income=max(s_df$income), area=sum(s_df$Area))

collect(d_df)


s_df %>>%
  groupBy("state_division") %>>%
  avg("Area", "Income") %>>%
  collect()
```

```{r}

registerTempTable(s_df, "s_table")

s2_df <-
  sql(hiveContext,
      "SELECT state_division, Population, Income * Population AS total_income FROM s_table")

s2_df %>>%
  groupBy(.$state_division) %>>%
  agg(total_income=sum(s2_dttotal_$income)) 
```


# 2. Examples

## Example with Parquet


```{r}
train_df <-
  sqlContext %>>%
  read.df("data/trainParquet")

test_df <-
  sqlContext %>>%
  read.df("data/testParquet")


train_df %>>%
  printSchema
````

## 2.2. Exploratory Analysis

- Question 1: How many are stores there?

```{r DistinctStores}
df1 %>>%
  select("Store") %>>% distinct %>>% count
```

- Question 2: Which day of the week has the highest number of promotions?

```{r DistinctStores2}
df1 %>>% filter("Promo = 1") %>>% groupBy("DayOfWeek") %>>% agg(Promo ="sum") %>>% collect
  
##  agg(df, age = "sum") 
  
##  select("Store") %>>% distinct %>>% count
```

# ML

```{r}
library(MASS)

boston_df <- createDataFrame(hiveContext, Boston)
lm.fit <- glm(medv~lstat,
              data=boston_df,
              family = "gaussian")

summary(lm.fit)


```
library(ISLR)

# Simple Linear Regression


lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))

# Multiple Linear Regression

lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
library(car)
vif(lm.fit)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit, ~.-age)


## 2.4. UNION

```{r}
#' <br>
#' Construimos una segunda funci??n `unir`, que haga el `merge` que necesitamos, pero que primero
#' renombre una de las columnas `user_id` y as?? podamos obtener el `DataFrame` de la uni??n
#' pero sin repetir esta columna:

unir <- function(x,y){
  y_aux  <- y %>>% withColumnRenamed("user_id","user_id_aux")
  unido  <- x %>>% merge(y_aux,x$user_id==y_aux$user_id_aux)
  quiero <- setdiff(names(unido),"user_id_aux")
  unido %>>% select(as.list(quiero))
}

#' Probamos su funcionamiento:
#unir(lista_df[[1]],lista_df[[2]])

```


## 2.5. kafh

```{r}
#' Con `Reduce` usamos la funci??n `unir` de manera recursiva para todos los
#'  elementos de la lista.

# df_unido <- Reduce(unir,lista_df) %>>% cache
# df_unido
# df_unido %>>% count()
# df_unido %>>% head

```


## 2.6. Exercices

```{r}
#' Para terminar, podemos usar `crosstab` para hacer una tabla de conteo. Por ejemplo entre
#' las predicciones del modelo1 y las del modelo2, y despu??s hacer un gr??fico:

# conteo <- df_unido %>% crosstab("modelo1_predict","modelo2_predict")
# conteo
# 
# #' Manipulamos el `data.frame` local para usar la funci??n `mosaic`:
# 
# rownames(conteo) <- paste0("modelo1_",conteo[,1])
# conteo <- conteo[,-1]
# colnames(conteo) <- paste0("modelo2_",colnames(conteo))
# 
# #' ordeno:
# conteo<-conteo[order(rownames(conteo)),order(colnames(conteo))]
# conteo
# 
# #' y grafico:
# mosaicplot(conteo,color=TRUE,main = "modelo1 vs modelo2")

```


## 2.3 Machine Learning


# 3.Conclussions

```{r, echo=FALSE}
sparkR.stop()
```

